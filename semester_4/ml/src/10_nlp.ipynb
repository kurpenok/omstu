{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329af9e3-e16c-4bf8-b592-d271920dabee",
   "metadata": {},
   "source": [
    "# 10. Основы обработки естественного языка (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b50ed-59bf-402b-ad1d-7230539fede8",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1931de-3048-4be2-9d85-cfa9ab3b6a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import tarfile\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from io import BytesIO\n",
    "from typing import Iterable, NamedTuple\n",
    "\n",
    "import requests\n",
    "import spacy\n",
    "import tqdm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from spacy.lookups import Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0366dd-4369-4fc8-9d2d-916e27258366",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf48aed-da2b-4c42-ab44-16db6ca70609",
   "metadata": {},
   "source": [
    "## Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ec70f7-3826-4170-adfd-a2919c93b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e828d549-b4e8-4f0d-b015-16fdcd832d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    \"20021010_easy_ham.tar.bz2\",\n",
    "    \"20021010_hard_ham.tar.bz2\",\n",
    "    \"20021010_spam.tar.bz2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac012e5-cd33-4ed8-b327-91a179c71407",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../data/spam_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c524ce17-3d7b-41cf-a520-10ed39747184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in FILES:\n",
    "#     content = requests.get(f\"{BASE_URL}/{filename}\").content\n",
    "#     fin = BytesIO(content)\n",
    "#     with tarfile.open(fileobj=fin, mode=\"r:bz2\") as tf:\n",
    "#         tf.extractall(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e4828e-4836-4abf-a6be-da8d0eb23d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/spam_data/*/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be6d580e-01b8-41bc-8ab4-be4a59589d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc49d160-e910-43b1-a2ca-d39165bb9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_subjects: list[Message] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8349ae02-e5bc-4919-95c0-e7b926d7d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(path):\n",
    "    with open(filename, errors=\"ignore\") as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                raw_subjects.append(Message(subject, is_spam=\"ham\" not in filename))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4358eade-d13e-4291-a7c5-3b1c1b00c506",
   "metadata": {},
   "source": [
    "## Работа с текстом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae8e507-ce79-475e-84aa-955b21497aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621c4d2-dfde-43c1-aa4d-4e9450f25fe3",
   "metadata": {},
   "source": [
    "#### Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e807463-a198-4584-8d3a-9a089c3de525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[+] Tokenizing: |██████████████████████████████████████████████████|\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "for subject in tqdm.tqdm(raw_subjects, bar_format=\"[+] Tokenizing: |{bar:50}|\"):\n",
    "    tokens.extend([token for token in nlp(subject.text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c10b78a8-adeb-4171-bff1-72c866621a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Lenght of tokens list: 29861\n"
     ]
    }
   ],
   "source": [
    "print(f\"[+] Lenght of tokens list: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368893f2-414e-4d5b-854c-a24bf281a19f",
   "metadata": {},
   "source": [
    "#### Удаление стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a425653c-5d89-4294-9ecc-317c8577879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nlp.Defaults.stop_words\n",
    "stopwords.add(\"re\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8afda87-8a01-48ae-9014-548950cc6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[+] Removing stop-words: |██████████████████████████████████████████████████|\n"
     ]
    }
   ],
   "source": [
    "cleared_tokens = []\n",
    "\n",
    "for token in tqdm.tqdm(tokens, bar_format=\"[+] Removing stop-words: |{bar:50}|\"):\n",
    "    if token.lower_ not in stopwords:\n",
    "        cleared_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd26d70-8d82-430d-8aeb-fa21e8a5f5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Lenght of cleared tokens list: 23773\n"
     ]
    }
   ],
   "source": [
    "print(f\"[+] Lenght of cleared tokens list: {len(cleared_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f92ad-2cb4-4757-9dae-eacaf5655dfd",
   "metadata": {},
   "source": [
    "#### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d9be55e-de3f-421d-83d1-44e805f91eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[+] Lemmatizing: |██████████████████████████████████████████████████|\n"
     ]
    }
   ],
   "source": [
    "lemmatized_tokens = []\n",
    "\n",
    "for token in tqdm.tqdm(cleared_tokens, bar_format=\"[+] Lemmatizing: |{bar:50}|\"):\n",
    "    lemmatized_tokens.append(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9252ca69-2974-44af-966a-ab7ec6587bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_tokens = lemmatized_tokens[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf68b71-31e5-48e8-8a82-3a9b119bfe9c",
   "metadata": {},
   "source": [
    "#### Выделение тем писем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "346ee390-1ba3-4221-b8ef-a209ee0acc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_texts = [subject.text for subject in raw_subjects]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b8c53-19b5-411e-98a1-b5c3ec9af528",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "342481e4-3d4a-4099-94a4-102dc3314cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98cc4f85-3820-4f0c-9678-90b2f14679a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = count_vectorizer.fit_transform(subject_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de4cd79e-f1d8-4b05-9d6b-5801f94524a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['re', 'the', 'curse', 'of', 'india']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(count_vectorizer.vocabulary_.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c97f627b-8afc-4190-972e-fc079785299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(texts: list[str]):\n",
    "    vocabulary = {}\n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(vocabulary)\n",
    "    \n",
    "    feature_matrix = []\n",
    "    for text in texts:\n",
    "        vector = [0] * len(vocabulary)\n",
    "        for word in text.split():\n",
    "            if word in vocabulary:\n",
    "                index = vocabulary[word]\n",
    "                vector[index] += 1\n",
    "        feature_matrix.append(vector)\n",
    "    \n",
    "    return feature_matrix, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89d2873f-f5a3-4482-b08c-9d5d575c5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, vocabulary = bag_of_words(subject_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e07c2a-3f7b-4a91-89cf-52f4952e24a5",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b08b226b-5f47-40e4-9ce0-bdb3ca5eac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c1d18e3-1221-412e-a055-526e4fc45073",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer.fit_transform(subject_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e9f4260-3620-498a-86a7-423d3249fa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['re', 'the', 'curse', 'of', 'india']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tfidf_vectorizer.vocabulary_.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a426d953-1350-4a4d-8f82-7d563cfd1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(text):\n",
    "    tf_dict = {}\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        tf_dict[word] = tf_dict.get(word, 0) + 1\n",
    "    # Нормализация TF\n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] /= len(words)\n",
    "    return tf_dict\n",
    "\n",
    "\n",
    "def compute_idf(texts):\n",
    "    idf_dict = {}\n",
    "    total_documents = len(texts)\n",
    "    \n",
    "    for text in texts:\n",
    "        words = set(text.split())\n",
    "        for word in words:\n",
    "            idf_dict[word] = idf_dict.get(word, 0) + 1\n",
    "    \n",
    "    for word in idf_dict:\n",
    "        idf_dict[word] = math.log(total_documents / idf_dict[word])\n",
    "    \n",
    "    return idf_dict\n",
    "\n",
    "\n",
    "def compute_tf_idf(texts):\n",
    "    tf_idf_matrix = []\n",
    "    idf_dict = compute_idf(texts)\n",
    "    \n",
    "    for text in texts:\n",
    "        tf_dict = compute_tf(text)\n",
    "        tf_idf_dict = {}\n",
    "        for word, tf_value in tf_dict.items():\n",
    "            tf_idf_dict[word] = tf_value * idf_dict.get(word, 0)\n",
    "        tf_idf_matrix.append(tf_idf_dict)\n",
    "    \n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca405526-2fad-43a1-bde7-df904bf75325",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = compute_tf_idf(subject_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705e2ee-c8ac-48dc-8857-ee04b6ac2838",
   "metadata": {},
   "source": [
    "## Классификация спама с помощью наивного Байеса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3424b1-90c6-4cf9-b2bd-df62437d7bb8",
   "metadata": {},
   "source": [
    "#### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20ac5272-3a95-41f0-9f5c-18b50151a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> set[str]:\n",
    "    text = text.lower()\n",
    "    all_words = re.findall(\"[a-z0-9']+\", text)\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4727a7e-816d-41a7-aead-0da31d25434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k\n",
    "        self.tokens: set[str] = set()\n",
    "        self.token_spam_counts: dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = 0\n",
    "        self.ham_messages = 0\n",
    "\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "    def _probabilities(self, token: str) -> tuple[float, float]:\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = 0.0\n",
    "        log_prob_if_ham = 0.0\n",
    "\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd3817-78b4-434a-8e5b-4dab89d7980e",
   "metadata": {},
   "source": [
    "#### Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed1d3eb8-74f1-48c7-a841-fe9998b1dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data: list[Message] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "193c9d3c-3b26-4e64-b253-324ffae3ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename\n",
    "\n",
    "    with open(filename, errors=\"ignore\") as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append(Message(subject, is_spam))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ac6ad0-160f-404f-8dc3-0ee7364bacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data: list[str], prob: float) -> tuple[list[str], list[str]]:\n",
    "    data = data[:]\n",
    "    random.shuffle(data)\n",
    "    cut = int(len(data) * prob)\n",
    "    return data[:cut], data[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33571a27-cc95-4c8d-8876-a99df6da9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages, test_messages = split_data(data, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa227894-2f8a-4688-b614-55332aea414c",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7851a944-427f-4442-9da2-b3592bd83a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayesClassifier()\n",
    "model.train(train_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af793737-236e-4900-ad37-45025fc2a105",
   "metadata": {},
   "source": [
    "#### Оценивание классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50a82b95-af6e-4020-9f93-865c05cf11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [(message, model.predict(message.text)) for message in test_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01766acd-8080-4ffb-9138-436274bb933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5) for message, spam_probability in predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e035b4ef-8aa1-4d82-8c90-dfa6c00dd3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Confusion matrix: Counter({(False, False): 675, (True, True): 83, (True, False): 44, (False, True): 23})\n"
     ]
    }
   ],
   "source": [
    "print(f\"[+] Confusion matrix: {confusion_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd87677d-7342-4317-bfd3-3941c099d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confusion_matrix[(True, True)]\n",
    "tn = confusion_matrix[(False, False)]\n",
    "fp = confusion_matrix[(True, False)]\n",
    "fn = confusion_matrix[(False, True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b16a5e7-e8db-45db-b42a-c9244846dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9563bd58-3fb2-465b-b1aa-1cbb5edf0d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Accuracy: 0.9188\n",
      "[+] Precision: 0.6535\n",
      "[+] Recall: 0.7830\n",
      "[+] F1-score: 0.7124\n"
     ]
    }
   ],
   "source": [
    "print(f\"[+] Accuracy: {accuracy:.4f}\")\n",
    "print(f\"[+] Precision: {precision:.4f}\")\n",
    "print(f\"[+] Recall: {recall:.4f}\")\n",
    "print(f\"[+] F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8007a31-8fef-421e-92b1-81bb3e5a57c9",
   "metadata": {},
   "source": [
    "#### Вероятностный анализ тем писем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e352cc5a-87a0-4ac7-94ca-37aa76bf58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float:\n",
    "    prob_if_spam, prob_if_ham = model._probabilities(token)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c26e586c-fd6b-41d4-8b14-5d2cb77d4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "867f4c81-7dc7-4cbb-ab85-24f7f3283954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Most spammy words: ['account', 'attn', 'zzzz', 'clearance', 'b', 'sale', 'rates', 'adv', 'systemworks', 'money']\n",
      "[+] Least spam words: ['satalk', 'spambayes', 'users', 'razor', 'zzzzteana', 'sadev', 'perl', 'apt', 'spamassassin', 'ouch']\n"
     ]
    }
   ],
   "source": [
    "print(f\"[+] Most spammy words: {words[-10:]}\")\n",
    "print(f\"[+] Least spam words: {words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3070a0f9-6665-4a2c-b193-e5c886554e82",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c3aaf22-af88-403f-b7d7-49daf0ad8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74aac67e-4f71-485f-add7-b5f7063d51c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        data = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(f\"[+] Topic {i}: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "897c1580-db54-4392-b519-cbb643566ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(min_df=2, max_df=0.95, max_features=n_features)\n",
    "tf = tf_vectorizer.fit_transform(subject_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6491b368-7b14-4857-952d-e1c03026858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=20, learning_method=\"online\", learning_offset=50).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "064f5126-82f9-427b-8cbd-fde86e5d3d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Topic 0: re for is from the spam sadev java kiddies bug\n",
      "[+] Topic 1: on the re spambayes zzzzteana are you new test low\n",
      "[+] Topic 2: re the was of ouch in bliss selling wedded message\n",
      "[+] Topic 3: re and razor users with satalk of problem it this\n",
      "[+] Topic 4: for 2002 use perl no headlines 10 09 dvd stories\n",
      "[+] Topic 5: alsa easy made almost spambayes more insurance life to 500\n",
      "[+] Topic 6: your re lockergnome tech sed daily united roman states empire\n",
      "[+] Topic 7: to the in re you news get at of can\n",
      "[+] Topic 8: re new my window me best sequences help please package\n",
      "[+] Topic 9: ilug re the cvs has internet rates not in spamassassin\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tf_vectorizer.get_feature_names_out(), 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
